#!/usr/bin/env python3
"""
æµ‹è¯•å®¢æˆ·ç«¯ - æ¨¡æ‹Ÿå‹æµ‹å·¥å…·ä¸ŠæŠ¥æ•°æ®
ä½¿ç”¨æ–¹æ³•ï¼š
python test_client.py --team-id team1 --team-name "ç¬¬ä¸€å°ç»„"
"""

import requests
import json
import time
import random
import argparse
from datetime import datetime
import sys
import io
import urllib.parse

# ä¿®å¤ç»ˆç«¯è¾“å‡ºç¼–ç é—®é¢˜
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def generate_mock_stats():
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„å‹æµ‹ç»Ÿè®¡æ•°æ®"""
    
    # åŸºç¡€æ•°æ®
    total_sent = random.randint(1000, 10000)
    total_ops = int(total_sent * random.uniform(0.85, 0.98))
    total_errors = total_sent - total_ops
    total_save_delay_errors = int(total_errors * random.uniform(0.1, 0.3))
    pending = random.randint(0, 100)
    total_elapsed = random.uniform(30, 300)  # 30ç§’åˆ°5åˆ†é’Ÿ
    
    # æ“ä½œç»Ÿè®¡
    operations = {
        "sensorData": {
            "operations": int(total_ops * random.uniform(0.3, 0.5)),
            "errors": int(total_errors * random.uniform(0.2, 0.4))
        },
        "sensorRW": {
            "operations": int(total_ops * random.uniform(0.2, 0.3)),
            "errors": int(total_errors * random.uniform(0.1, 0.3))
        },
        "batchRW": {
            "operations": int(total_ops * random.uniform(0.1, 0.2)),
            "errors": int(total_errors * random.uniform(0.1, 0.2))
        },
        "query": {
            "operations": int(total_ops * random.uniform(0.1, 0.2)),
            "errors": int(total_errors * random.uniform(0.1, 0.2))
        }
    }
    
    # é«˜ä¼˜å…ˆçº§ç»Ÿè®¡
    high_priority_total = int(total_sent * random.uniform(0.1, 0.3))
    high_priority_stats = {
        "sensorDataCount": int(high_priority_total * random.uniform(0.3, 0.5)),
        "sensorRWCount": int(high_priority_total * random.uniform(0.2, 0.3)),
        "batchRWCount": int(high_priority_total * random.uniform(0.1, 0.2)),
        "queryCount": int(high_priority_total * random.uniform(0.1, 0.2)),
        "totalCount": high_priority_total,
        "percentage": (high_priority_total / total_sent) * 100
    }
    
    # æ€§èƒ½æŒ‡æ ‡
    performance_metrics = {
        "avgSentQPS": total_sent / total_elapsed,
        "avgCompletedQPS": total_ops / total_elapsed,
        "errorRate": (total_errors / total_ops) * 100 if total_ops > 0 else 0
    }
    
    # å»¶è¿Ÿåˆ†æ
    def generate_latency_distribution():
        avg_latency = random.uniform(10, 500)
        min_latency = avg_latency * random.uniform(0.1, 0.5)
        max_latency = avg_latency * random.uniform(2, 10)
        
        # ç”Ÿæˆå»¶è¿Ÿåˆ†å¸ƒæ¡¶
        buckets = [random.randint(0, 100) for _ in range(13)]  # 13ä¸ªæ¡¶
        
        # é«˜ä¼˜å…ˆçº§å»¶è¿Ÿç»Ÿè®¡
        high_priority_count = random.randint(0, 100)
        high_priority_avg = avg_latency * random.uniform(0.5, 1.5) if high_priority_count > 0 else None
        high_priority_min = min_latency * random.uniform(0.5, 1.0) if high_priority_count > 0 else None
        high_priority_max = max_latency * random.uniform(0.5, 1.5) if high_priority_count > 0 else None
        high_priority_buckets = [random.randint(0, 50) for _ in range(13)] if high_priority_count > 0 else None
        
        return {
            "avg": avg_latency,
            "min": min_latency,
            "max": max_latency,
            "buckets": buckets,
            "highPriorityCount": high_priority_count,
            "highPriorityAvg": high_priority_avg,
            "highPriorityMin": high_priority_min,
            "highPriorityMax": high_priority_max,
            "highPriorityBuckets": high_priority_buckets
        }
    
    latency_analysis = {
        "sensorData": generate_latency_distribution(),
        "sensorRW": generate_latency_distribution(),
        "batchRW": generate_latency_distribution(),
        "query": generate_latency_distribution()
    }
    
    return {
        "totalElapsed": total_elapsed,
        "totalSent": total_sent,
        "totalOps": total_ops,
        "totalErrors": total_errors,
        "totalSaveDelayErrors": total_save_delay_errors,
        "pending": pending,
        "operations": operations,
        "highPriorityStats": high_priority_stats,
        "performanceMetrics": performance_metrics,
        "latencyAnalysis": latency_analysis
    }

def submit_stats(server_url, team_id, team_name, stats):
    """æäº¤ç»Ÿè®¡æ•°æ®åˆ°æœåŠ¡å™¨"""
    # å¯¹ä¸­æ–‡å›¢é˜Ÿåè¿›è¡ŒURLç¼–ç ï¼Œé¿å…HTTPå¤´éƒ¨ç¼–ç é—®é¢˜
    encoded_team_name = urllib.parse.quote(team_name, safe='')
    
    headers = {
        'Content-Type': 'application/json; charset=utf-8',
        'Accept-Charset': 'utf-8',
        'X-Team-ID': team_id,
        'X-Team-Name': encoded_team_name
    }
    
    try:
        url = f"{server_url}/api/stats/report"
        response = requests.post(
            url,
            headers=headers,
            data=json.dumps(stats, ensure_ascii=False).encode('utf-8'),
            timeout=10
        )
        if response.status_code == 200:
            print(f"âœ… æ•°æ®æäº¤æˆåŠŸ: {response.json()}")
            return True
        else:
            print(f"âŒ æ•°æ®æäº¤å¤±è´¥: {response.status_code} - {response.text}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œé”™è¯¯: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='BenchBoard æµ‹è¯•å®¢æˆ·ç«¯')
    parser.add_argument('--team-id', required=True, help='å›¢é˜ŸID')
    parser.add_argument('--team-name', required=True, help='å›¢é˜Ÿåç§°')
    parser.add_argument('--server', default='http://localhost:8080', help='æœåŠ¡å™¨åœ°å€')
    parser.add_argument('--interval', type=int, default=30, help='ä¸ŠæŠ¥é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--count', type=int, default=0, help='ä¸ŠæŠ¥æ¬¡æ•°ï¼ˆ0è¡¨ç¤ºæ— é™å¾ªç¯ï¼‰')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ å¯åŠ¨æµ‹è¯•å®¢æˆ·ç«¯")
    print(f"   å›¢é˜ŸID: {args.team_id}")
    print(f"   å›¢é˜Ÿåç§°: {args.team_name}")
    print(f"   æœåŠ¡å™¨åœ°å€: {args.server}")
    print(f"   ä¸ŠæŠ¥é—´éš”: {args.interval}ç§’")
    print(f"   ä¸ŠæŠ¥æ¬¡æ•°: {'æ— é™å¾ªç¯' if args.count == 0 else args.count}")
    print("-" * 50)
    
    count = 0
    while True:
        count += 1
        print(f"\nğŸ“Š ç¬¬ {count} æ¬¡ä¸ŠæŠ¥ - {datetime.now().strftime('%H:%M:%S')}")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        stats = generate_mock_stats()
        
        # æäº¤æ•°æ®
        success = submit_stats(args.server, args.team_id, args.team_name, stats)
        
        if success:
            print(f"   æ€»è¿è¡Œæ—¶é—´: {stats['totalElapsed']:.1f}s")
            print(f"   å‘é€è¯·æ±‚: {stats['totalSent']}")
            print(f"   å®Œæˆè¯·æ±‚: {stats['totalOps']}")
            print(f"   é”™è¯¯æ•°: {stats['totalErrors']}")
            print(f"   å¹³å‡QPS: {stats['performanceMetrics']['avgCompletedQPS']:.1f}")
            print(f"   é”™è¯¯ç‡: {stats['performanceMetrics']['errorRate']:.2f}%")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æŒ‡å®šæ¬¡æ•°
        if args.count > 0 and count >= args.count:
            print(f"\nâœ… å®Œæˆ {args.count} æ¬¡ä¸ŠæŠ¥")
            break
        
        # ç­‰å¾…ä¸‹æ¬¡ä¸ŠæŠ¥
        if args.interval > 0:
            print(f"â³ ç­‰å¾… {args.interval} ç§’...")
            time.sleep(args.interval)

if __name__ == '__main__':
    main() 