import os
import json
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from pydantic import BaseModel, Field
from dataclasses import dataclass
import threading
import time
import urllib.parse
import glob
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

app = Flask(__name__)
app.config['SECRET_KEY'] = 'benchboard-secret-key'
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ•°æ®å­˜å‚¨
@dataclass
class TeamData:
    team_id: str
    team_name: str
    last_update: datetime
    stats: Dict
    is_active: bool = True
    best_qps: float = 0.0
    best_qps_time: Optional[datetime] = None
    best_latency: float = float('inf')
    best_latency_time: Optional[datetime] = None

# ç¼“å­˜ç»“æ„
@dataclass 
class TeamCache:
    file_count: int = 0
    last_scan_time: Optional[datetime] = None
    best_records_cached: bool = False
    file_mtime_hash: str = ""

# å…¨å±€å­˜å‚¨
teams_data: Dict[str, TeamData] = {}
team_cache: Dict[str, TeamCache] = {}
data_lock = threading.Lock()
cache_lock = threading.Lock()

# æ€§èƒ½é…ç½®
CACHE_EXPIRE_SECONDS = 300  # ç¼“å­˜5åˆ†é’Ÿè¿‡æœŸ
MAX_WORKER_THREADS = 4      # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
BATCH_SIZE = 50             # æ‰¹é‡å¤„ç†æ–‡ä»¶æ•°é‡

# Pydanticæ¨¡å‹å®šä¹‰
class OperationStat(BaseModel):
    operations: int = Field(..., description="æ“ä½œæ•°é‡")
    errors: int = Field(..., description="é”™è¯¯æ•°é‡")

class OperationsStats(BaseModel):
    sensorData: OperationStat

class HighPriorityStats(BaseModel):
    sensorDataCount: int = Field(..., description="ä¼ æ„Ÿå™¨æ•°æ®ä¸ŠæŠ¥é«˜ä¼˜å…ˆçº§è¯·æ±‚æ•°")
    totalCount: int = Field(..., description="é«˜ä¼˜å…ˆçº§è¯·æ±‚æ€»æ•°")
    percentage: float = Field(..., description="é«˜ä¼˜å…ˆçº§è¯·æ±‚å æ¯”ï¼ˆ%ï¼‰")

class PerformanceMetrics(BaseModel):
    avgSentQPS: float = Field(..., description="å¹³å‡å‘é€ QPS")
    avgCompletedQPS: float = Field(..., description="å¹³å‡å®Œæˆ QPS")
    errorRate: float = Field(..., description="é”™è¯¯ç‡ï¼ˆ%ï¼‰")

class LatencyDistribution(BaseModel):
    avg: float = Field(..., description="å¹³å‡å»¶è¿Ÿï¼ˆmsï¼‰")
    min: float = Field(..., description="æœ€å°å»¶è¿Ÿï¼ˆmsï¼‰")
    max: float = Field(..., description="æœ€å¤§å»¶è¿Ÿï¼ˆmsï¼‰")
    buckets: List[int] = Field(..., description="å»¶è¿Ÿåˆ†å¸ƒæ¡¶è®¡æ•°")
    highPriorityCount: Optional[int] = Field(None, description="é«˜ä¼˜å…ˆçº§è¯·æ±‚æ•°é‡")
    highPriorityAvg: Optional[float] = Field(None, description="é«˜ä¼˜å…ˆçº§å¹³å‡å»¶è¿Ÿï¼ˆmsï¼‰")
    highPriorityMin: Optional[float] = Field(None, description="é«˜ä¼˜å…ˆçº§æœ€å°å»¶è¿Ÿï¼ˆmsï¼‰")
    highPriorityMax: Optional[float] = Field(None, description="é«˜ä¼˜å…ˆçº§æœ€å¤§å»¶è¿Ÿï¼ˆmsï¼‰")
    highPriorityBuckets: Optional[List[int]] = Field(None, description="é«˜ä¼˜å…ˆçº§å»¶è¿Ÿåˆ†å¸ƒæ¡¶è®¡æ•°")

class LatencyAnalysis(BaseModel):
    sensorData: LatencyDistribution

class StatsReport(BaseModel):
    totalElapsed: float = Field(..., description="æ€»è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰")
    totalSent: int = Field(..., description="å‘é€è¯·æ±‚æ•°")
    totalOps: int = Field(..., description="å®Œæˆè¯·æ±‚æ•°")
    totalErrors: int = Field(..., description="æ€»é”™è¯¯æ•°")
    totalSaveDelayErrors: int = Field(..., description="æ€»å› ä¸ºå‘ç°è½ç›˜æ—¶é—´è¶…æ—¶è€Œäº§ç”Ÿçš„é”™è¯¯æ•°")
    totalAvgLatency: Optional[float] = Field(None, description="æ€»å¹³å‡å»¶è¿Ÿï¼ˆmsï¼‰")
    highPriorityAvgDelayLatency: Optional[float] = Field(None, description="é«˜ä¼˜å…ˆçº§å¹³å‡å»¶è¿Ÿï¼ˆmsï¼‰")
    totalVerifyErrorRate: Optional[float] = Field(None, description="æ€»éªŒè¯é”™è¯¯ç‡ï¼ˆ%ï¼‰")
    pending: int = Field(..., description="å¾…å¤„ç†è¯·æ±‚æ•°")
    operations: OperationsStats
    highPriorityStats: HighPriorityStats
    performanceMetrics: PerformanceMetrics
    latencyAnalysis: LatencyAnalysis

def calculate_p99_latency(buckets: List[int]) -> float:
    """è®¡ç®—P99å»¶è¿Ÿ"""
    bucket_boundaries = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, float('inf')]
    total_requests = sum(buckets)
    if total_requests == 0:
        return 0.0
    
    p99_threshold = total_requests * 0.99
    cumulative = 0
    
    for i, count in enumerate(buckets):
        cumulative += count
        if cumulative >= p99_threshold:
            if i == 0:
                return bucket_boundaries[i]
            else:
                # çº¿æ€§æ’å€¼
                prev_cumulative = cumulative - count
                ratio = (p99_threshold - prev_cumulative) / count
                lower_bound = bucket_boundaries[i-1] if i > 0 else 0
                upper_bound = bucket_boundaries[i]
                return lower_bound + ratio * (upper_bound - lower_bound)
    
    return bucket_boundaries[-2]  # è¿”å›æœ€åä¸€ä¸ªæœ‰é™è¾¹ç•Œ

def calculate_data_loss_rate(stats: Dict) -> float:
    """è®¡ç®—å¤±è´¥ç‡"""
    total_sent = stats.get('totalSent', 0)
    total_ops = stats.get('totalOps', 0)
    pending = stats.get('pending', 0)
    
    if total_sent == 0:
        return 0.0
    
    # æ•°æ®ä¸¢å¤± = å‘é€æ•° - å®Œæˆæ•° - å¾…å¤„ç†æ•°
    lost = total_sent - total_ops - pending
    return (lost / total_sent) * 100 if lost > 0 else 0.0

def calculate_overall_metrics(stats: Dict) -> Dict:
    """è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
    # ä¼˜å…ˆä½¿ç”¨æ–°çš„å­—æ®µ
    avg_latency = stats.get('totalAvgLatency')
    high_priority_latency = stats.get('highPriorityAvgDelayLatency')
    
    # å¦‚æœæ–°å­—æ®µä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ—§çš„è®¡ç®—æ–¹å¼
    if avg_latency is None:
        latency_analysis = stats.get('latencyAnalysis', {})
        
        # ç°åœ¨åªå¤„ç†sensorDataæ“ä½œç±»å‹
        if 'sensorData' in latency_analysis:
            op_data = latency_analysis['sensorData']
            op_requests = sum(op_data.get('buckets', []))
            if op_requests > 0:
                avg_latency = op_data.get('avg', 0)
            else:
                avg_latency = 0
        else:
            avg_latency = 0
    
    # å¦‚æœé«˜ä¼˜å…ˆçº§å»¶è¿Ÿå­—æ®µä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ—§çš„è®¡ç®—æ–¹å¼
    if high_priority_latency is None:
        latency_analysis = stats.get('latencyAnalysis', {})
        
        # ç°åœ¨åªå¤„ç†sensorDataæ“ä½œç±»å‹
        if 'sensorData' in latency_analysis:
            op_data = latency_analysis['sensorData']
            if op_data.get('highPriorityAvg') and op_data.get('highPriorityCount'):
                high_priority_latency = op_data['highPriorityAvg']
            else:
                high_priority_latency = 0
        else:
            high_priority_latency = 0
    
    # è®¡ç®—æ€»ä½“P99å»¶è¿Ÿï¼ˆä½¿ç”¨sensorDataä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
    latency_analysis = stats.get('latencyAnalysis', {})
    sensor_buckets = latency_analysis.get('sensorData', {}).get('buckets', [])
    p99_latency = calculate_p99_latency(sensor_buckets)
    
    return {
        'avg_latency': avg_latency,
        'p99_latency': p99_latency,
        'high_priority_latency': high_priority_latency,
        'data_loss_rate': calculate_data_loss_rate(stats)
    }

def save_team_data(team_id: str, team_name: str, stats: Dict):
    """ä¿å­˜å›¢é˜Ÿæ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶ - æ¯æ¬¡ä¸ŠæŠ¥éƒ½å•ç‹¬å­˜å‚¨"""
    # åˆ›å»ºå›¢é˜Ÿç›®å½•
    team_dir = f"data/{team_id}"
    if not os.path.exists(team_dir):
        os.makedirs(team_dir)
    
    # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºæ–‡ä»¶å
    timestamp = datetime.now()
    timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç²¾ç¡®åˆ°æ¯«ç§’
    filename = f"{team_dir}/{timestamp_str}.json"
    
    data = {
        "team_id": team_id,
        "team_name": team_name,
        "timestamp": timestamp.isoformat(),
        "stats": stats
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # åŒæ—¶ä¿å­˜æœ€æ–°æ•°æ®åˆ° latest.json ä»¥ä¾¿å¿«é€Ÿè®¿é—®
    latest_filename = f"{team_dir}/latest.json"
    with open(latest_filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_team_data(team_id: str) -> Optional[Dict]:
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å›¢é˜Ÿæœ€æ–°æ•°æ®"""
    filename = f"data/{team_id}/latest.json"
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None

def get_file_mtime_hash(team_dir: str) -> str:
    """è·å–å›¢é˜Ÿç›®å½•æ–‡ä»¶ä¿®æ”¹æ—¶é—´çš„å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜å¤±æ•ˆæ£€æµ‹"""
    try:
        json_files = glob.glob(f"{team_dir}/*.json")
        if not json_files:
            return ""
        
        mtime_list = []
        for file_path in json_files:
            try:
                mtime = os.path.getmtime(file_path)
                mtime_list.append(f"{os.path.basename(file_path)}:{mtime}")
            except OSError:
                continue
        
        # åˆ›å»ºç®€å•çš„å“ˆå¸Œ
        content = "|".join(sorted(mtime_list))
        return str(hash(content))
    except Exception:
        return ""

def get_team_history_files_cached(team_id: str) -> List[Dict]:
    """è·å–å›¢é˜Ÿæ‰€æœ‰å†å²æ•°æ®æ–‡ä»¶åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
    team_dir = f"data/{team_id}"
    if not os.path.exists(team_dir):
        return []
    
    with cache_lock:
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        cache_entry = team_cache.get(team_id)
        current_mtime_hash = get_file_mtime_hash(team_dir)
        
        if (cache_entry and 
            cache_entry.file_mtime_hash == current_mtime_hash and
            cache_entry.last_scan_time and
            (datetime.now() - cache_entry.last_scan_time).total_seconds() < CACHE_EXPIRE_SECONDS):
            # ç¼“å­˜æœ‰æ•ˆï¼Œä»ç¼“å­˜è¯»å–
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶åˆ—è¡¨: {team_id}")
    
    # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°æ‰«æ
    history_files = []
    json_files = glob.glob(f"{team_dir}/*.json")
    
    # ä½¿ç”¨æ–‡ä»¶åè¿›è¡Œæ—¶é—´æˆ³è§£æï¼ˆé¿å…è¯»å–æ–‡ä»¶å†…å®¹ï¼‰
    for file_path in json_files:
        filename = os.path.basename(file_path)
        if filename == 'latest.json':
            continue
            
        # å°è¯•ä»æ–‡ä»¶åè§£ææ—¶é—´æˆ³
        try:
            # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º: 20250801_171514_920.json
            name_part = filename.replace('.json', '')
            if '_' in name_part and len(name_part) >= 17:
                timestamp_str = f"{name_part[:8]}T{name_part[9:11]}:{name_part[11:13]}:{name_part[13:15]}.{name_part[16:]}"
                timestamp = timestamp_str.replace('_', 'T').replace('T', '-', 2).replace('-', 'T', 1)
                history_files.append({
                    'filename': filename,
                    'timestamp': timestamp,
                    'file_path': file_path
                })
            else:
                # å›é€€åˆ°è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    history_files.append({
                        'filename': filename,
                        'timestamp': data.get('timestamp'),
                        'file_path': file_path
                    })
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            continue
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    history_files.sort(key=lambda x: x['timestamp'], reverse=True)
    
    # æ›´æ–°ç¼“å­˜
    with cache_lock:
        team_cache[team_id] = TeamCache(
            file_count=len(history_files),
            last_scan_time=datetime.now(),
            file_mtime_hash=current_mtime_hash
        )
    
    return history_files

def get_team_history_files(team_id: str) -> List[Dict]:
    """è·å–å›¢é˜Ÿæ‰€æœ‰å†å²æ•°æ®æ–‡ä»¶åˆ—è¡¨ï¼ˆå…¼å®¹æ€§åŒ…è£…ï¼‰"""
    return get_team_history_files_cached(team_id)

def load_team_history_data(team_id: str, limit: int = 10) -> List[Dict]:
    """åŠ è½½å›¢é˜Ÿå†å²æ•°æ®ï¼ˆé™åˆ¶æ•°é‡ï¼‰"""
    history_files = get_team_history_files(team_id)
    history_data = []
    
    for file_info in history_files[:limit]:
        try:
            with open(file_info['file_path'], 'r', encoding='utf-8') as f:
                data = json.load(f)
                history_data.append(data)
        except Exception as e:
            print(f"Error loading {file_info['file_path']}: {e}")
            continue
    
    return history_data

def load_single_team(team_dir_name: str) -> Optional[TeamData]:
    """åŠ è½½å•ä¸ªå›¢é˜Ÿæ•°æ®ï¼ˆç”¨äºå¹¶å‘åŠ è½½ï¼‰"""
    try:
        team_dir_path = os.path.join("data", team_dir_name)
        
        # è·³è¿‡éç›®å½•æ–‡ä»¶
        if not os.path.isdir(team_dir_path):
            return None
            
        latest_file = os.path.join(team_dir_path, "latest.json")
        
        # å¦‚æœlatest.jsonå­˜åœ¨ï¼ŒåŠ è½½å›¢é˜Ÿæ•°æ®
        if not os.path.exists(latest_file):
            return None
            
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        team_id = data.get('team_id', team_dir_name)
        team_name = data.get('team_name', f'Team-{team_id}')
        timestamp_str = data.get('timestamp')
        stats = data.get('stats', {})
        
        # è§£ææ—¶é—´æˆ³
        try:
            last_update = datetime.fromisoformat(timestamp_str) if timestamp_str else datetime.now()
        except:
            last_update = datetime.now()
        
        # è®¡ç®—æœ€ä½³è®°å½•
        best_qps, best_qps_time, best_latency, best_latency_time = calculate_best_records(team_id)
        
        # åˆ›å»ºTeamDataå¯¹è±¡
        team_data = TeamData(
            team_id=team_id,
            team_name=team_name,
            last_update=last_update,
            stats=stats,
            is_active=True,  # å¯åŠ¨æ—¶éƒ½è®¾ä¸ºæ´»è·ƒ
            best_qps=best_qps,
            best_qps_time=best_qps_time,
            best_latency=best_latency,
            best_latency_time=best_latency_time
        )
        
        logger.info(f"âœ… åŠ è½½å›¢é˜Ÿ: {team_name} (ID: {team_id})")
        logger.info(f"   æœ€åæ›´æ–°: {last_update.strftime('%Y-%m-%d %H:%M:%S')}")
        if best_latency is not None:
            logger.info(f"   æœ€ä½³QPS: {best_qps:.1f} | æœ€ä½³å»¶è¿Ÿ: {best_latency:.1f}ms")
        else:
            logger.info(f"   æœ€ä½³QPS: {best_qps:.1f} | æœ€ä½³å»¶è¿Ÿ: N/A")
        
        return team_data
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½å›¢é˜Ÿ {team_dir_name} å¤±è´¥: {e}")
        return None

def load_all_teams_on_startup():
    """å¯åŠ¨æ—¶ä»dataç›®å½•è¯»å–æ‰€æœ‰å›¢é˜Ÿæ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ”¯æŒå¹¶å‘åŠ è½½ï¼‰"""
    logger.info("ğŸ”„ å¯åŠ¨æ—¶åŠ è½½å†å²æ•°æ®...")
    
    data_dir = "data"
    if not os.path.exists(data_dir):
        logger.warning("ğŸ“ dataç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®åŠ è½½")
        return 0
    
    # è·å–æ‰€æœ‰å›¢é˜Ÿç›®å½•
    team_dirs = []
    try:
        for item in os.listdir(data_dir):
            team_dir_path = os.path.join(data_dir, item)
            if os.path.isdir(team_dir_path):
                team_dirs.append(item)
    except Exception as e:
        logger.error(f"æ‰«ædataç›®å½•å¤±è´¥: {e}")
        return 0
    
    if not team_dirs:
        logger.info("ğŸ“ dataç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°å›¢é˜Ÿæ•°æ®")
        return 0
    
    loaded_teams = 0
    failed_teams = []
    
    # ä½¿ç”¨å¹¶å‘åŠ è½½å›¢é˜Ÿæ•°æ®
    logger.info(f"ğŸ”„ å‘ç° {len(team_dirs)} ä¸ªå›¢é˜Ÿç›®å½•ï¼Œå¼€å§‹å¹¶å‘åŠ è½½...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:
        # æäº¤æ‰€æœ‰åŠ è½½ä»»åŠ¡
        future_to_team = {executor.submit(load_single_team, team_dir): team_dir for team_dir in team_dirs}
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_team):
            team_dir = future_to_team[future]
            try:
                team_data = future.result()
                if team_data:
                    with data_lock:
                        teams_data[team_data.team_id] = team_data
                    loaded_teams += 1
                else:
                    failed_teams.append(team_dir)
            except Exception as e:
                logger.error(f"å¤„ç†å›¢é˜Ÿ {team_dir} çš„ç»“æœæ—¶å¤±è´¥: {e}")
                failed_teams.append(team_dir)
    
    # è¾“å‡ºåŠ è½½ç»“æœ
    logger.info(f"ğŸ“Š å…±åŠ è½½ {loaded_teams} ä¸ªå›¢é˜Ÿçš„å†å²æ•°æ®")
    if failed_teams:
        logger.warning(f"âš ï¸ åŠ è½½å¤±è´¥çš„å›¢é˜Ÿ: {', '.join(failed_teams)}")
    
    # æ¸…ç†ç¼“å­˜çŠ¶æ€
    with cache_lock:
        for team_id in teams_data.keys():
            if team_id not in team_cache:
                team_cache[team_id] = TeamCache()
    
    return loaded_teams

def get_cache_status() -> Dict:
    """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰"""
    with cache_lock:
        cache_info = {}
        for team_id, cache_entry in team_cache.items():
            cache_info[team_id] = {
                'file_count': cache_entry.file_count,
                'last_scan_time': cache_entry.last_scan_time.isoformat() if cache_entry.last_scan_time else None,
                'best_records_cached': cache_entry.best_records_cached,
                'cache_age_seconds': (datetime.now() - cache_entry.last_scan_time).total_seconds() if cache_entry.last_scan_time else None
            }
        return cache_info

def clear_team_cache(team_id: str = None):
    """æ¸…ç†å›¢é˜Ÿç¼“å­˜"""
    with cache_lock:
        if team_id:
            if team_id in team_cache:
                del team_cache[team_id]
                logger.info(f"å·²æ¸…ç†å›¢é˜Ÿ {team_id} çš„ç¼“å­˜")
        else:
            team_cache.clear()
            logger.info("å·²æ¸…ç†æ‰€æœ‰å›¢é˜Ÿç¼“å­˜")

def calculate_best_records_batch(file_paths: List[str]) -> Tuple[float, datetime, float, datetime]:
    """æ‰¹é‡è®¡ç®—æœ€ä½³è®°å½•ï¼ˆç”¨äºå¹¶å‘å¤„ç†ï¼‰"""
    best_qps = 0.0
    best_qps_time = None
    best_latency = float('inf')
    best_latency_time = None
    
    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            stats = data.get('stats', {})
            timestamp_str = data.get('timestamp')
            
            # è§£ææ—¶é—´æˆ³
            try:
                timestamp = datetime.fromisoformat(timestamp_str) if timestamp_str else None
            except:
                timestamp = None
            
            # è·å–QPS
            current_qps = stats.get('performanceMetrics', {}).get('avgCompletedQPS', 0)
            if current_qps > best_qps:
                best_qps = current_qps
                best_qps_time = timestamp
            
            # è®¡ç®—å½“å‰å»¶è¿Ÿ
            current_metrics = calculate_overall_metrics(stats)
            current_latency = current_metrics.get('avg_latency', 0)
            
            # æ›´æ–°æœ€ä½³å»¶è¿Ÿï¼ˆæ›´å°çš„æ›´å¥½ï¼‰
            if current_latency > 0 and current_latency < best_latency:
                best_latency = current_latency
                best_latency_time = timestamp
                
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–å†å²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            continue
    
    return best_qps, best_qps_time, best_latency, best_latency_time

def calculate_best_records(team_id: str) -> Tuple[float, datetime, float, datetime]:
    """è®¡ç®—å›¢é˜Ÿçš„æœ€ä½³è®°å½•ï¼ˆQPSå’Œå»¶è¿Ÿï¼‰- ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # æ£€æŸ¥ç¼“å­˜
    with cache_lock:
        cache_entry = team_cache.get(team_id)
        if (cache_entry and cache_entry.best_records_cached and
            cache_entry.last_scan_time and
            (datetime.now() - cache_entry.last_scan_time).total_seconds() < CACHE_EXPIRE_SECONDS):
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æœ€ä½³è®°å½•: {team_id}")
            # è¿™é‡Œåº”è¯¥è¿”å›ç¼“å­˜çš„å€¼ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬é‡æ–°è®¡ç®—
    
    best_qps = 0.0
    best_qps_time = None
    best_latency = float('inf')
    best_latency_time = None
    
    try:
        # è·å–æ‰€æœ‰å†å²æ–‡ä»¶
        history_files = get_team_history_files(team_id)
        
        if not history_files:
            return best_qps, best_qps_time, None, None
        
        file_paths = [f['file_path'] for f in history_files]
        
        # å¦‚æœæ–‡ä»¶æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥å¤„ç†
        if len(file_paths) <= BATCH_SIZE:
            best_qps, best_qps_time, best_latency, best_latency_time = calculate_best_records_batch(file_paths)
        else:
            # æ–‡ä»¶æ•°é‡è¾ƒå¤šï¼Œä½¿ç”¨å¹¶å‘å¤„ç†
            logger.info(f"å›¢é˜Ÿ {team_id} æœ‰ {len(file_paths)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨å¹¶å‘å¤„ç†")
            
            # åˆ†æ‰¹å¤„ç†
            batches = [file_paths[i:i + BATCH_SIZE] for i in range(0, len(file_paths), BATCH_SIZE)]
            
            with ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:
                future_to_batch = {executor.submit(calculate_best_records_batch, batch): batch for batch in batches}
                
                for future in as_completed(future_to_batch):
                    try:
                        batch_qps, batch_qps_time, batch_latency, batch_latency_time = future.result()
                        
                        # åˆå¹¶ç»“æœ
                        if batch_qps > best_qps:
                            best_qps = batch_qps
                            best_qps_time = batch_qps_time
                        
                        if batch_latency < best_latency and batch_latency != float('inf'):
                            best_latency = batch_latency
                            best_latency_time = batch_latency_time
                            
                    except Exception as e:
                        logger.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
                        continue
        
        # æ›´æ–°ç¼“å­˜
        with cache_lock:
            if team_id in team_cache:
                team_cache[team_id].best_records_cached = True
                
    except Exception as e:
        logger.error(f"âš ï¸ è®¡ç®—æœ€ä½³è®°å½•å¤±è´¥ (team_id: {team_id}): {e}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å»¶è¿Ÿæ•°æ®ï¼Œè®¾ä¸ºNone
    if best_latency == float('inf'):
        best_latency = None
        best_latency_time = None
    
    return best_qps, best_qps_time, best_latency, best_latency_time

@app.route('/')
def dashboard():
    """Webçœ‹æ¿é¡µé¢"""
    return render_template('dashboard.html')

@app.route('/team/<team_id>/history')
def team_history_page(team_id):
    """å›¢é˜Ÿå†å²æ•°æ®æŸ¥çœ‹é¡µé¢"""
    # è·å–å›¢é˜ŸåŸºæœ¬ä¿¡æ¯
    with data_lock:
        if team_id in teams_data:
            team_data = teams_data[team_id]
            team_name = team_data.team_name
        else:
            # å°è¯•ä»æ–‡ä»¶åŠ è½½
            file_data = load_team_data(team_id)
            team_name = file_data.get('team_name', f'Team-{team_id}') if file_data else f'Team-{team_id}'
    
    return render_template('team_history.html', team_id=team_id, team_name=team_name)

@app.route('/api/stats/report', methods=['POST'])
def submit_stats():
    """æ¥æ”¶å‹æµ‹ç»“æœä¸ŠæŠ¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No data provided"}), 400
        
        # éªŒè¯æ•°æ®æ ¼å¼
        stats_report = StatsReport(**data)
        
        # è·å–å›¢é˜Ÿä¿¡æ¯
        team_id = request.headers.get('X-Team-ID')
        encoded_team_name = request.headers.get('X-Team-Name', f'Team-{team_id}')
        
        # è§£ç URLç¼–ç çš„å›¢é˜Ÿåç§°
        try:
            team_name = urllib.parse.unquote(encoded_team_name)
        except:
            team_name = encoded_team_name  # å¦‚æœè§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å€¼
        
        if not team_id:
            return jsonify({"error": "X-Team-ID header is required"}), 400
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        current_qps = data.get('performanceMetrics', {}).get('avgCompletedQPS', 0)
        current_metrics = calculate_overall_metrics(data)
        current_latency = current_metrics['avg_latency']
        
        # ä¿å­˜æ•°æ®
        with data_lock:
            if team_id in teams_data:
                team_data = teams_data[team_id]
                # æ›´æ–°æœ€ä½³QPSè®°å½•
                if current_qps > team_data.best_qps:
                    team_data.best_qps = current_qps
                    team_data.best_qps_time = datetime.now()
                
                # æ›´æ–°æœ€ä½³å»¶è¿Ÿè®°å½•
                if current_latency > 0 and current_latency < team_data.best_latency:
                    team_data.best_latency = current_latency
                    team_data.best_latency_time = datetime.now()
                
                # æ›´æ–°å…¶ä»–ä¿¡æ¯
                team_data.team_name = team_name
                team_data.last_update = datetime.now()
                team_data.stats = data
                team_data.is_active = True
            else:
                teams_data[team_id] = TeamData(
                    team_id=team_id,
                    team_name=team_name,
                    last_update=datetime.now(),
                    stats=data,
                    best_qps=current_qps,
                    best_qps_time=datetime.now() if current_qps > 0 else None,
                    best_latency=current_latency if current_latency > 0 else float('inf'),
                    best_latency_time=datetime.now() if current_latency > 0 else None
                )
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        save_team_data(team_id, team_name, data)
        
        # é€šè¿‡WebSocketå¹¿æ’­æ›´æ–°
        socketio.emit('stats_update', {
            'team_id': team_id,
            'team_name': team_name,
            'stats': data,
            'timestamp': datetime.now().isoformat(),
            'metrics': current_metrics,
            'best_qps': teams_data[team_id].best_qps,
            'best_qps_time': teams_data[team_id].best_qps_time.isoformat() if teams_data[team_id].best_qps_time else None,
            'best_latency': teams_data[team_id].best_latency if teams_data[team_id].best_latency != float('inf') else None,
            'best_latency_time': teams_data[team_id].best_latency_time.isoformat() if teams_data[team_id].best_latency_time else None
        })
        
        return jsonify({"message": "Stats submitted successfully", "team_id": team_id}), 200
        
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route('/api/teams', methods=['GET'])
def get_teams():
    """è·å–æ‰€æœ‰å›¢é˜Ÿæ•°æ®"""
    with data_lock:
        teams_list = []
        for team_id, team_data in teams_data.items():
            teams_list.append({
                'team_id': team_id,
                'team_name': team_data.team_name,
                'last_update': team_data.last_update.isoformat(),
                'is_active': team_data.is_active
            })
        return jsonify(teams_list)

@app.route('/api/teams/<team_id>', methods=['GET'])
def get_team_stats(team_id):
    """è·å–ç‰¹å®šå›¢é˜Ÿçš„ç»Ÿè®¡æ•°æ®"""
    with data_lock:
        if team_id in teams_data:
            team_data = teams_data[team_id]
            metrics = calculate_overall_metrics(team_data.stats)
            return jsonify({
                'team_id': team_id,
                'team_name': team_data.team_name,
                'last_update': team_data.last_update.isoformat(),
                'stats': team_data.stats,
                'metrics': metrics,
                'best_qps': team_data.best_qps,
                'best_qps_time': team_data.best_qps_time.isoformat() if team_data.best_qps_time else None,
                'best_latency': team_data.best_latency if team_data.best_latency != float('inf') else None,
                'best_latency_time': team_data.best_latency_time.isoformat() if team_data.best_latency_time else None
            })
        else:
            return jsonify({"error": "Team not found"}), 404

@app.route('/api/teams/<team_id>/history', methods=['GET'])
def get_team_history(team_id):
    """è·å–å›¢é˜Ÿå†å²æ•°æ®"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        limit = int(request.args.get('limit', 10))  # é»˜è®¤è¿”å›10æ¡
        offset = int(request.args.get('offset', 0))  # é»˜è®¤ä»0å¼€å§‹
        
        # è·å–å†å²æ–‡ä»¶åˆ—è¡¨
        history_files = get_team_history_files(team_id)
        
        if not history_files:
            return jsonify({
                "message": "No history found for this team",
                "team_id": team_id,
                "history": [],
                "total": 0
            }), 200
        
        # åº”ç”¨åˆ†é¡µ
        total = len(history_files)
        paginated_files = history_files[offset:offset + limit]
        
        # åŠ è½½æ•°æ®
        history_data = []
        for file_info in paginated_files:
            try:
                with open(file_info['file_path'], 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # æ·»åŠ è®¡ç®—çš„æ€§èƒ½æŒ‡æ ‡
                    metrics = calculate_overall_metrics(data.get('stats', {}))
                    data['metrics'] = metrics
                    history_data.append(data)
            except Exception as e:
                print(f"Error loading {file_info['file_path']}: {e}")
                continue
        
        return jsonify({
            "team_id": team_id,
            "history": history_data,
            "total": total,
            "limit": limit,
            "offset": offset,
            "has_more": offset + limit < total
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route('/api/teams/<team_id>/history/summary', methods=['GET'])
def get_team_history_summary(team_id):
    """è·å–å›¢é˜Ÿå†å²æ•°æ®æ‘˜è¦ï¼ˆæ–‡ä»¶æ•°é‡ã€æ—¶é—´èŒƒå›´ç­‰ï¼‰"""
    try:
        history_files = get_team_history_files(team_id)
        
        if not history_files:
            return jsonify({
                "team_id": team_id,
                "total_reports": 0,
                "first_report": None,
                "last_report": None,
                "data_directory": f"data/{team_id}"
            })
        
        return jsonify({
            "team_id": team_id,
            "total_reports": len(history_files),
            "first_report": history_files[-1]['timestamp'] if history_files else None,
            "last_report": history_files[0]['timestamp'] if history_files else None,
            "data_directory": f"data/{team_id}",
            "recent_files": [f['filename'] for f in history_files[:5]]  # æœ€è¿‘5ä¸ªæ–‡ä»¶
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route('/api/cache/status', methods=['GET'])
def get_cache_status_api():
    """è·å–ç¼“å­˜çŠ¶æ€API"""
    try:
        cache_status = get_cache_status()
        total_teams = len(teams_data)
        cached_teams = len([c for c in team_cache.values() if c.best_records_cached])
        
        return jsonify({
            "cache_summary": {
                "total_teams": total_teams,
                "cached_teams": cached_teams,
                "cache_hit_rate": (cached_teams / total_teams * 100) if total_teams > 0 else 0
            },
            "team_cache_details": cache_status,
            "config": {
                "cache_expire_seconds": CACHE_EXPIRE_SECONDS,
                "max_worker_threads": MAX_WORKER_THREADS,
                "batch_size": BATCH_SIZE
            }
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route('/api/cache/clear', methods=['POST'])
def clear_cache_api():
    """æ¸…ç†ç¼“å­˜API"""
    try:
        team_id = request.json.get('team_id') if request.json else None
        clear_team_cache(team_id)
        
        if team_id:
            return jsonify({"message": f"å›¢é˜Ÿ {team_id} ç¼“å­˜å·²æ¸…ç†"})
        else:
            return jsonify({"message": "æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†"})
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@socketio.on('connect')
def handle_connect():
    """å®¢æˆ·ç«¯è¿æ¥æ—¶å‘é€å½“å‰æ•°æ®"""
    with data_lock:
        for team_id, team_data in teams_data.items():
            metrics = calculate_overall_metrics(team_data.stats)
            emit('stats_update', {
                'team_id': team_id,
                'team_name': team_data.team_name,
                'stats': team_data.stats,
                'timestamp': team_data.last_update.isoformat(),
                'metrics': metrics,
                'best_qps': team_data.best_qps,
                'best_qps_time': team_data.best_qps_time.isoformat() if team_data.best_qps_time else None,
                'best_latency': team_data.best_latency if team_data.best_latency != float('inf') else None,
                'best_latency_time': team_data.best_latency_time.isoformat() if team_data.best_latency_time else None
            })

def cleanup_inactive_teams():
    """æ¸…ç†ä¸æ´»è·ƒçš„å›¢é˜Ÿï¼ˆè¶…è¿‡5åˆ†é’Ÿæ²¡æœ‰æ›´æ–°ï¼‰"""
    while True:
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        current_time = datetime.now()
        with data_lock:
            inactive_teams = []
            for team_id, team_data in teams_data.items():
                if (current_time - team_data.last_update).total_seconds() > 300:  # 5åˆ†é’Ÿ
                    team_data.is_active = False
                    inactive_teams.append(team_id)
            
            # ç§»é™¤ä¸æ´»è·ƒçš„å›¢é˜Ÿ
            for team_id in inactive_teams:
                del teams_data[team_id]

if __name__ == '__main__':
    # å¯åŠ¨æ—¶é—´è®°å½•
    start_time = datetime.now()
    
    # # å¯åŠ¨æ¸…ç†çº¿ç¨‹
    # cleanup_thread = threading.Thread(target=cleanup_inactive_teams, daemon=True)
    # cleanup_thread.start()
    
    logger.info("ğŸš€ å¯åŠ¨ BenchBoard æœåŠ¡å™¨...")
    logger.info(f"   é…ç½®: ç¼“å­˜è¿‡æœŸ={CACHE_EXPIRE_SECONDS}s, æœ€å¤§çº¿ç¨‹={MAX_WORKER_THREADS}, æ‰¹å¤§å°={BATCH_SIZE}")
    
    try:
        # å¯åŠ¨æ—¶åŠ è½½å†å²æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        loaded_count = load_all_teams_on_startup()
        
        # è¾“å‡ºåŠ è½½ç»Ÿè®¡
        load_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {loaded_count} ä¸ªå›¢é˜Ÿ, è€—æ—¶ {load_time:.2f}s")
        
        if loaded_count > 0:
            logger.info(f"ğŸ’¾ ç¼“å­˜çŠ¶æ€: {len(team_cache)} ä¸ªå›¢é˜Ÿå·²ç¼“å­˜")
        
        # å¯åŠ¨æœåŠ¡å™¨
        logger.info("ğŸŒ æœåŠ¡å™¨å¯åŠ¨ä¸­... (0.0.0.0:8080)")
        socketio.run(app, host='0.0.0.0', port=8080, debug=True, allow_unsafe_werkzeug=True)
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        raise 
    